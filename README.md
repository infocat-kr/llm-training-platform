# ğŸ§  LLM Training Platform

It is a comprehensive platform for creating and training large-scale language models (LLMs).
I asked a large-scale language model to write it on Cursor

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

- ğŸ—ï¸ **Various model architectures**: GPT, BERT, T5 style Transformer model support
- ğŸ“š **Data processing**: Text preprocessing, Tokenizing, Dataset managing
- ğŸš€ **High-performance training**: Mixed Precision, Gradient Accumulation, Learning Rate Scheduling
- ğŸ¯ **Inference engine**: Text creation, chat, embedding extraction
- ğŸŒ **Web interface**: User-friendly UI based on Flask and Gradio
- ğŸ“Š **Monitoring**: Real-time training monitoring, metric visualization
- âš™ï¸ **Setting management**: YAML-based hyperparameter management
## ğŸš€ Quick start

### 1. Installing

```bash
# Repository clone
git clone <repository-url>
cd llm_training_platform

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Dependency installation
pip install -r requirements.txt
```

### 2. Example execution

```bash
# Example script execution
./run_example.sh

# Or individual execution
python train.py --train-data data/sample_data.txt --epochs 5
python inference.py --model-path outputs/best_model.pt --input-text "Hello world"
```

### 3. Web interface execution

```bash
# Flask Web application
python app.py
# ì ‘ì†: http://localhost:5000

# Gradio Web application
python gradio_app.py
# ì ‘ì†: http://localhost:7860
```

## ğŸ“ Project structure

```
llm_training_platform/
â”œâ”€â”€ models/              # ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚   â”œâ”€â”€ transformer.py   # Transformer ëª¨ë¸ë“¤
â”‚   â”œâ”€â”€ attention.py     # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
â”‚   â””â”€â”€ layers.py        # ê¸°ë³¸ ë ˆì´ì–´ë“¤
â”œâ”€â”€ data/                # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ tokenizer.py     # í† í¬ë‚˜ì´ì €
â”‚   â”œâ”€â”€ dataset.py       # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
â”‚   â””â”€â”€ preprocessing.py # ì „ì²˜ë¦¬
â”œâ”€â”€ training/            # í›ˆë ¨ ë¡œì§
â”‚   â”œâ”€â”€ trainer.py       # í›ˆë ¨ê¸°
â”‚   â”œâ”€â”€ optimizer.py     # ì˜µí‹°ë§ˆì´ì €
â”‚   â””â”€â”€ loss.py          # ì†ì‹¤ í•¨ìˆ˜
â”œâ”€â”€ inference/           # ì¶”ë¡  ì—”ì§„
â”‚   â”œâ”€â”€ generator.py     # í…ìŠ¤íŠ¸ ìƒì„±
â”‚   â”œâ”€â”€ chat.py          # ì±„íŒ…ë´‡
â”‚   â””â”€â”€ evaluator.py     # ëª¨ë¸ í‰ê°€
â”œâ”€â”€ web/                 # ì›¹ ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ app.py           # Flask ì•±
â”‚   â”œâ”€â”€ gradio_app.py    # Gradio ì•±
â”‚   â””â”€â”€ templates/       # HTML í…œí”Œë¦¿
â”œâ”€â”€ configs/             # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ config.py        # ì„¤ì • í´ë˜ìŠ¤
â”‚   â””â”€â”€ default.yaml     # ê¸°ë³¸ ì„¤ì •
â”œâ”€â”€ utils/               # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ helpers.py       # í—¬í¼ í•¨ìˆ˜
â”‚   â”œâ”€â”€ logging.py       # ë¡œê¹…
â”‚   â””â”€â”€ visualization.py # ì‹œê°í™”
â”œâ”€â”€ notebooks/           # Jupyter ë…¸íŠ¸ë¶
â”œâ”€â”€ data/                # ë°ì´í„° íŒŒì¼
â””â”€â”€ outputs/             # ì¶œë ¥ íŒŒì¼
```

## ğŸ¯ ì‚¬ìš© ë°©ë²•

### ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤

#### ëª¨ë¸ í›ˆë ¨

```bash
python train.py \
    --train-data data/train.txt \
    --val-data data/val.txt \
    --model-type gpt \
    --d-model 512 \
    --num-layers 6 \
    --num-heads 8 \
    --epochs 10 \
    --batch-size 32 \
    --learning-rate 1e-4 \
    --output-dir outputs/my_model
```

#### ëª¨ë¸ ì¶”ë¡ 

```bash
python inference.py \
    --model-path outputs/my_model/best_model.pt \
    --input-text "Once upon a time" \
    --max-length 100 \
    --temperature 0.8 \
    --top-p 0.9
```

### Python API

```python
from models import TransformerModel
from data import SimpleTokenizer, TextDataset
from training import Trainer, TrainingConfig

# ëª¨ë¸ ìƒì„±
model = TransformerModel(
    model_type='gpt',
    vocab_size=30000,
    d_model=512,
    num_heads=8,
    num_layers=6
)

# í† í¬ë‚˜ì´ì € ìƒì„±
tokenizer = SimpleTokenizer(vocab_size=30000)
tokenizer.build_vocab(texts)

# ë°ì´í„°ì…‹ ìƒì„±
dataset = TextDataset(texts, tokenizer, max_length=512)

# í›ˆë ¨ ì‹¤í–‰
trainer = Trainer(model, TrainingConfig(), train_loader)
trainer.train()
```

### ì›¹ ì¸í„°í˜ì´ìŠ¤

1. **Flask ì›¹ ì•±**: ì™„ì „í•œ ê¸°ëŠ¥ì˜ ì›¹ ì¸í„°í˜ì´ìŠ¤
   - ëª¨ë¸ í›ˆë ¨ ë° ëª¨ë‹ˆí„°ë§
   - í…ìŠ¤íŠ¸ ìƒì„± ë° ì±„íŒ…
   - ì„¤ì • ê´€ë¦¬

2. **Gradio ì•±**: ê°„ë‹¨í•œ ë°ëª¨ ì¸í„°í˜ì´ìŠ¤
   - ëŒ€í™”í˜• ëª¨ë¸ í›ˆë ¨
   - ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìƒì„±
   - ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

## âš™ï¸ ì„¤ì •

### ê¸°ë³¸ ì„¤ì • (configs/default.yaml)

```yaml
model:
  model_type: 'gpt'
  d_model: 512
  num_layers: 6
  num_heads: 8
  vocab_size: 30000

training:
  epochs: 10
  batch_size: 32
  learning_rate: 1e-4
  optimizer_type: 'adamw'
  scheduler_type: 'cosine'

inference:
  max_length: 100
  temperature: 1.0
  top_p: 0.9
```

### ëª¨ë¸ë³„ í”„ë¦¬ì…‹

- `gpt_small.yaml`: ì‘ì€ GPT ëª¨ë¸ (256D, 4L)
- `gpt_medium.yaml`: ì¤‘ê°„ GPT ëª¨ë¸ (512D, 6L)
- `gpt_large.yaml`: í° GPT ëª¨ë¸ (768D, 12L)
- `bert_base.yaml`: BERT ë² ì´ìŠ¤ ëª¨ë¸
- `t5_small.yaml`: T5 ìŠ¤ëª° ëª¨ë¸

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì‹œê°í™”

### í›ˆë ¨ ëª¨ë‹ˆí„°ë§

- ì‹¤ì‹œê°„ ì†ì‹¤ ë° ë©”íŠ¸ë¦­ ì¶”ì 
- Weights & Biases í†µí•©
- TensorBoard ì§€ì›
- ì»¤ìŠ¤í…€ ë¡œê¹…

### ì‹œê°í™” ë„êµ¬

```python
from utils.visualization import plot_training_curves, plot_attention_weights

# í›ˆë ¨ ê³¡ì„  ì‹œê°í™”
plot_training_curves('logs/training.json', save_plots=True)

# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
plot_attention_weights(attention_weights, tokens, layer=0, head=0)
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### Mixed Precision í›ˆë ¨

```bash
python train.py --mixed-precision
```

### Gradient Checkpointing

```bash
python train.py --gradient-checkpointing
```

### Weights & Biases ë¡œê¹…

```bash
python train.py --use-wandb --wandb-project my-project
```

### ë‹¤ì¤‘ GPU í›ˆë ¨

```python
# DataParallel ì‚¬ìš©
model = torch.nn.DataParallel(model)

# DistributedDataParallel ì‚¬ìš© (ê³ ê¸‰)
# torch.distributed.launch ë˜ëŠ” torchrun ì‚¬ìš©
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ ìµœì í™”

- Gradient Accumulation
- Gradient Checkpointing
- Mixed Precision Training
- ëª¨ë¸ ë³‘ë ¬í™”

### ì†ë„ ìµœì í™”

- ì»´íŒŒì¼ëœ ëª¨ë¸ (torch.compile)
- ìµœì í™”ëœ ë°ì´í„°ë¡œë”
- íš¨ìœ¨ì ì¸ ì–´í…ì…˜ êµ¬í˜„

## ğŸ§ª ì‹¤í—˜ ë° í‰ê°€

### ëª¨ë¸ í‰ê°€

```python
from inference import ModelEvaluator

evaluator = ModelEvaluator(model, tokenizer)
metrics = evaluator.evaluate_language_model(test_data)
print(f"Perplexity: {metrics['perplexity']:.2f}")
```

### ë²¤ì¹˜ë§ˆí‚¹

```python
from utils.helpers import benchmark_model

benchmark_results = benchmark_model(model, (batch_size, seq_len))
print(f"Throughput: {benchmark_results['throughput']:.2f} samples/sec")
```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ğŸ™ ê°ì‚¬ì˜ ë§

- PyTorch íŒ€
- Hugging Face Transformers
- OpenAI GPT ë…¼ë¬¸
- Google BERT ë…¼ë¬¸
- ëª¨ë“  ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ìë“¤

## ğŸ“ ì§€ì›

- Issues: GitHub Issues ì‚¬ìš©
- ë¬¸ì„œ: í”„ë¡œì íŠ¸ Wiki ì°¸ì¡°
- í† ë¡ : GitHub Discussions ì‚¬ìš©

---

**LLM Training Platform**ìœ¼ë¡œ ë‹¹ì‹ ë§Œì˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”! ğŸš€
