"""
Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤
"""

import gradio as gr
import torch
import sys
import os
from typing import List, Tuple, Optional
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import TransformerModel
from data import SimpleTokenizer, TextDataset, create_dataloader
from training import Trainer, TrainingConfig
from inference import InferenceEngine, InferenceConfig, TextGenerator, GenerationConfig, ChatBot, ChatConfig
from configs import load_config, Config


class GradioApp:
    """Gradio ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.inference_engine = None
        self.chatbot = None
        self.config = Config()
        
    def create_interface(self):
        """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        
        with gr.Blocks(
            title="LLM Training Platform",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                max-width: 1200px !important;
            }
            .chat-message {
                padding: 10px;
                margin: 5px 0;
                border-radius: 10px;
            }
            .user-message {
                background-color: #007bff;
                color: white;
                margin-left: 20%;
            }
            .ai-message {
                background-color: #f8f9fa;
                color: black;
                margin-right: 20%;
            }
            """
        ) as app:
            
            gr.Markdown("""
            # ğŸ§  LLM Training Platform
            
            ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“¤ê³  í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆëŠ” ì¢…í•©ì ì¸ í”Œë«í¼ì…ë‹ˆë‹¤.
            """)
            
            with gr.Tabs():
                # ëª¨ë¸ í›ˆë ¨ íƒ­
                with gr.Tab("ğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨"):
                    self.create_training_interface()
                
                # í…ìŠ¤íŠ¸ ìƒì„± íƒ­
                with gr.Tab("âœ¨ í…ìŠ¤íŠ¸ ìƒì„±"):
                    self.create_generation_interface()
                
                # ì±„íŒ… íƒ­
                with gr.Tab("ğŸ’¬ AI ì±„íŒ…"):
                    self.create_chat_interface()
                
                # ëª¨ë¸ ê´€ë¦¬ íƒ­
                with gr.Tab("âš™ï¸ ëª¨ë¸ ê´€ë¦¬"):
                    self.create_model_management_interface()
        
        return app
        
    def create_training_interface(self):
        """í›ˆë ¨ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### í›ˆë ¨ ì„¤ì •")
                
                model_type = gr.Dropdown(
                    choices=["gpt", "bert", "t5"],
                    value="gpt",
                    label="ëª¨ë¸ íƒ€ì…"
                )
                
                d_model = gr.Slider(
                    minimum=128, maximum=2048, value=512, step=128,
                    label="ëª¨ë¸ ì°¨ì› (d_model)"
                )
                
                num_layers = gr.Slider(
                    minimum=2, maximum=24, value=6, step=1,
                    label="ë ˆì´ì–´ ìˆ˜"
                )
                
                num_heads = gr.Slider(
                    minimum=1, maximum=16, value=8, step=1,
                    label="ì–´í…ì…˜ í—¤ë“œ ìˆ˜"
                )
                
                batch_size = gr.Slider(
                    minimum=1, maximum=128, value=32, step=1,
                    label="ë°°ì¹˜ í¬ê¸°"
                )
                
                epochs = gr.Slider(
                    minimum=1, maximum=100, value=10, step=1,
                    label="ì—í¬í¬ ìˆ˜"
                )
                
                learning_rate = gr.Slider(
                    minimum=1e-6, maximum=1e-2, value=1e-4, step=1e-5,
                    label="í•™ìŠµë¥ "
                )
                
                train_data = gr.File(
                    label="í›ˆë ¨ ë°ì´í„° (í…ìŠ¤íŠ¸ íŒŒì¼)",
                    file_types=[".txt"]
                )
                
                val_data = gr.File(
                    label="ê²€ì¦ ë°ì´í„° (í…ìŠ¤íŠ¸ íŒŒì¼)",
                    file_types=[".txt"]
                )
                
                start_training_btn = gr.Button(
                    "í›ˆë ¨ ì‹œì‘",
                    variant="primary",
                    size="lg"
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### í›ˆë ¨ ì§„í–‰ ìƒí™©")
                
                training_status = gr.Textbox(
                    label="ìƒíƒœ",
                    value="ëŒ€ê¸° ì¤‘...",
                    interactive=False
                )
                
                training_progress = gr.Progress()
                
                training_log = gr.Textbox(
                    label="í›ˆë ¨ ë¡œê·¸",
                    lines=15,
                    interactive=False,
                    show_copy_button=True
                )
                
                # í›ˆë ¨ í†µê³„
                with gr.Row():
                    current_epoch = gr.Number(label="í˜„ì¬ ì—í¬í¬", value=0)
                    train_loss = gr.Number(label="í›ˆë ¨ ì†ì‹¤", value=0.0)
                    val_loss = gr.Number(label="ê²€ì¦ ì†ì‹¤", value=0.0)
                    learning_rate_current = gr.Number(label="í˜„ì¬ í•™ìŠµë¥ ", value=0.0)
        
        # í›ˆë ¨ ì‹œì‘ ì´ë²¤íŠ¸
        start_training_btn.click(
            fn=self.start_training,
            inputs=[
                model_type, d_model, num_layers, num_heads,
                batch_size, epochs, learning_rate, train_data, val_data
            ],
            outputs=[
                training_status, training_progress, training_log,
                current_epoch, train_loss, val_loss, learning_rate_current
            ]
        )
        
    def create_generation_interface(self):
        """í…ìŠ¤íŠ¸ ìƒì„± ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ìƒì„± ì„¤ì •")
                
                max_length = gr.Slider(
                    minimum=10, maximum=1000, value=100, step=10,
                    label="ìµœëŒ€ ê¸¸ì´"
                )
                
                temperature = gr.Slider(
                    minimum=0.1, maximum=2.0, value=1.0, step=0.1,
                    label="ì˜¨ë„ (Temperature)"
                )
                
                top_p = gr.Slider(
                    minimum=0.1, maximum=1.0, value=0.9, step=0.1,
                    label="Top-p (Nucleus Sampling)"
                )
                
                top_k = gr.Slider(
                    minimum=1, maximum=100, value=50, step=1,
                    label="Top-k"
                )
                
                repetition_penalty = gr.Slider(
                    minimum=1.0, maximum=2.0, value=1.1, step=0.1,
                    label="ë°˜ë³µ í˜ë„í‹°"
                )
                
                num_sequences = gr.Slider(
                    minimum=1, maximum=5, value=1, step=1,
                    label="ìƒì„±í•  ì‹œí€€ìŠ¤ ìˆ˜"
                )
                
                do_sample = gr.Checkbox(
                    label="ìƒ˜í”Œë§ ì‚¬ìš©",
                    value=True
                )
                
                generate_btn = gr.Button(
                    "í…ìŠ¤íŠ¸ ìƒì„±",
                    variant="primary",
                    size="lg"
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### ì…ë ¥ ë° ê²°ê³¼")
                
                input_text = gr.Textbox(
                    label="ì…ë ¥ í…ìŠ¤íŠ¸ (í”„ë¡¬í”„íŠ¸)",
                    lines=4,
                    placeholder="ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì‹œì‘ ë¶€ë¶„ì„ ì…ë ¥í•˜ì„¸ìš”...",
                    value="Once upon a time, in a distant galaxy, there was a small planet where"
                )
                
                generated_text = gr.Textbox(
                    label="ìƒì„±ëœ í…ìŠ¤íŠ¸",
                    lines=10,
                    interactive=False,
                    show_copy_button=True
                )
                
                # ìƒì„± í†µê³„
                with gr.Row():
                    generation_time = gr.Number(label="ìƒì„± ì‹œê°„ (ì´ˆ)", value=0.0)
                    generated_tokens = gr.Number(label="ìƒì„±ëœ í† í° ìˆ˜", value=0)
                    tokens_per_second = gr.Number(label="í† í°/ì´ˆ", value=0.0)
        
        # í…ìŠ¤íŠ¸ ìƒì„± ì´ë²¤íŠ¸
        generate_btn.click(
            fn=self.generate_text,
            inputs=[
                input_text, max_length, temperature, top_p, top_k,
                repetition_penalty, num_sequences, do_sample
            ],
            outputs=[generated_text, generation_time, generated_tokens, tokens_per_second]
        )
        
    def create_chat_interface(self):
        """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ì±„íŒ… ì„¤ì •")
                
                system_prompt = gr.Textbox(
                    label="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                    lines=3,
                    value="You are a helpful AI assistant. Please provide accurate and helpful responses.",
                    placeholder="AIì˜ ì—­í• ì„ ì •ì˜í•˜ì„¸ìš”..."
                )
                
                chat_temperature = gr.Slider(
                    minimum=0.1, maximum=2.0, value=0.7, step=0.1,
                    label="ì˜¨ë„"
                )
                
                max_response_length = gr.Slider(
                    minimum=50, maximum=1000, value=200, step=10,
                    label="ìµœëŒ€ ì‘ë‹µ ê¸¸ì´"
                )
                
                max_history = gr.Slider(
                    minimum=1, maximum=50, value=10, step=1,
                    label="ëŒ€í™” íˆìŠ¤í† ë¦¬"
                )
                
                enable_memory = gr.Checkbox(
                    label="ì¥ê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©",
                    value=True
                )
                
                clear_chat_btn = gr.Button(
                    "ëŒ€í™” ì´ˆê¸°í™”",
                    variant="secondary"
                )
                
            with gr.Column(scale=2):
                gr.Markdown("### AI ì±„íŒ…")
                
                chatbot = gr.Chatbot(
                    label="ëŒ€í™”",
                    height=400,
                    show_copy_button=True
                )
                
                with gr.Row():
                    msg = gr.Textbox(
                        label="ë©”ì‹œì§€",
                        placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                        scale=4
                    )
                    send_btn = gr.Button(
                        "ì „ì†¡",
                        variant="primary",
                        scale=1
                    )
                
                # ì±„íŒ… í†µê³„
                with gr.Row():
                    total_messages = gr.Number(label="ì´ ë©”ì‹œì§€ ìˆ˜", value=0)
                    avg_response_time = gr.Number(label="í‰ê·  ì‘ë‹µ ì‹œê°„ (ì´ˆ)", value=0.0)
        
        # ì±„íŒ… ì´ë²¤íŠ¸
        def respond(message, history):
            if not message.strip():
                return history, ""
            
            # AI ì‘ë‹µ ìƒì„±
            response = self.chat_response(message, history)
            history.append([message, response])
            
            return history, ""
        
        msg.submit(respond, [msg, chatbot], [chatbot, msg])
        send_btn.click(respond, [msg, chatbot], [chatbot, msg])
        
        clear_chat_btn.click(
            fn=lambda: ([], 0, 0.0),
            outputs=[chatbot, total_messages, avg_response_time]
        )
        
    def create_model_management_interface(self):
        """ëª¨ë¸ ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ëª¨ë¸ ë¡œë“œ")
                
                model_path = gr.Textbox(
                    label="ëª¨ë¸ ê²½ë¡œ",
                    placeholder="./outputs/best_model.pt"
                )
                
                config_path = gr.Textbox(
                    label="ì„¤ì • íŒŒì¼ ê²½ë¡œ",
                    placeholder="./configs/default.yaml"
                )
                
                load_model_btn = gr.Button(
                    "ëª¨ë¸ ë¡œë“œ",
                    variant="primary"
                )
                
                model_status = gr.Textbox(
                    label="ëª¨ë¸ ìƒíƒœ",
                    value="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    interactive=False
                )
                
            with gr.Column():
                gr.Markdown("### ëª¨ë¸ ì •ë³´")
                
                model_info = gr.JSON(
                    label="ëª¨ë¸ ì •ë³´",
                    value={}
                )
                
                gr.Markdown("### ì‹œìŠ¤í…œ ìƒíƒœ")
                
                system_info = gr.JSON(
                    label="ì‹œìŠ¤í…œ ì •ë³´",
                    value={}
                )
        
        # ëª¨ë¸ ë¡œë“œ ì´ë²¤íŠ¸
        load_model_btn.click(
            fn=self.load_model,
            inputs=[model_path, config_path],
            outputs=[model_status, model_info, system_info]
        )
        
    def start_training(self, model_type, d_model, num_layers, num_heads, 
                      batch_size, epochs, learning_rate, train_data, val_data):
        """í›ˆë ¨ ì‹œì‘"""
        
        try:
            # í›ˆë ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
            yield "í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...", gr.Progress(0.1), "í›ˆë ¨ ì´ˆê¸°í™” ì¤‘...", 0, 0.0, 0.0, 0.0
            
            # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
            self.model = TransformerModel(
                model_type=model_type,
                vocab_size=30000,
                d_model=int(d_model),
                num_heads=int(num_heads),
                num_layers=int(num_layers),
                d_ff=int(d_model) * 4,
                max_len=1024,
                dropout=0.1
            )
            
            # í† í¬ë‚˜ì´ì € ìƒì„±
            self.tokenizer = SimpleTokenizer(vocab_size=30000)
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ í›ˆë ¨ ì§„í–‰
            for epoch in range(int(epochs)):
                # í›ˆë ¨ ì§„í–‰ë¥  ê³„ì‚°
                progress = (epoch + 1) / epochs
                
                # ì‹œë®¬ë ˆì´ì…˜ëœ ì†ì‹¤ê°’
                train_loss_val = 2.5 - (epoch * 0.2) + (torch.rand(1).item() * 0.1)
                val_loss_val = 2.6 - (epoch * 0.18) + (torch.rand(1).item() * 0.1)
                lr_current = learning_rate * (0.95 ** epoch)
                
                log_message = f"Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss_val:.4f}, Val Loss: {val_loss_val:.4f}, LR: {lr_current:.6f}"
                
                yield (
                    f"í›ˆë ¨ ì¤‘... (ì—í¬í¬ {epoch + 1}/{epochs})",
                    gr.Progress(progress),
                    log_message,
                    epoch + 1,
                    train_loss_val,
                    val_loss_val,
                    lr_current
                )
                
                time.sleep(0.5)  # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì§€ì—°
            
            yield "í›ˆë ¨ ì™„ë£Œ!", gr.Progress(1.0), "í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", epochs, train_loss_val, val_loss_val, lr_current
            
        except Exception as e:
            yield f"í›ˆë ¨ ì˜¤ë¥˜: {str(e)}", gr.Progress(0.0), f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 0, 0.0, 0.0, 0.0
    
    def generate_text(self, input_text, max_length, temperature, top_p, top_k, 
                     repetition_penalty, num_sequences, do_sample):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        
        if self.model is None or self.tokenizer is None:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ê±°ë‚˜ í›ˆë ¨í•˜ì„¸ìš”.", 0.0, 0, 0.0
        
        try:
            start_time = time.time()
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ì‹œë®¬ë ˆì´ì…˜
            generated_text = f"{input_text} the inhabitants lived in harmony with nature. They had developed advanced technologies that were in perfect balance with their environment. The planet was known throughout the galaxy for its unique approach to sustainable living and peaceful coexistence."
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # í† í° ìˆ˜ ì¶”ì •
            token_count = len(generated_text.split())
            tokens_per_second = token_count / generation_time if generation_time > 0 else 0
            
            return generated_text, generation_time, token_count, tokens_per_second
            
        except Exception as e:
            return f"ìƒì„± ì˜¤ë¥˜: {str(e)}", 0.0, 0, 0.0
    
    def chat_response(self, message, history):
        """ì±„íŒ… ì‘ë‹µ ìƒì„±"""
        
        if self.model is None or self.tokenizer is None:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ê±°ë‚˜ í›ˆë ¨í•˜ì„¸ìš”."
        
        try:
            # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„± ì‹œë®¬ë ˆì´ì…˜
            responses = [
                "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                "í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì´ë„¤ìš”. ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì‹œê² ì–´ìš”?",
                "ê·¸ê²ƒì— ëŒ€í•´ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ ì •ë³´ê°€ ìˆìœ¼ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.",
                "ì¢‹ì€ ì§€ì ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ê´€ì ì—ì„œë„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.",
                "ì´í•´í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”."
            ]
            
            # ë©”ì‹œì§€ ê¸¸ì´ì— ë”°ë¼ ì‘ë‹µ ì„ íƒ
            response_index = len(message) % len(responses)
            return responses[response_index]
            
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def load_model(self, model_path, config_path):
        """ëª¨ë¸ ë¡œë“œ"""
        
        try:
            if not model_path:
                return "ëª¨ë¸ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", {}, {}
            
            # ì„¤ì • ë¡œë“œ
            if config_path and os.path.exists(config_path):
                self.config = load_config(config_path)
            else:
                self.config = Config()
            
            # ëª¨ë¸ ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜
            self.model = TransformerModel(
                model_type=self.config.model.model_type,
                vocab_size=self.config.model.vocab_size,
                d_model=self.config.model.d_model,
                num_heads=self.config.model.num_heads,
                num_layers=self.config.model.num_layers,
                d_ff=self.config.model.d_ff,
                max_len=self.config.model.max_length,
                dropout=self.config.model.dropout
            )
            
            self.tokenizer = SimpleTokenizer(vocab_size=self.config.model.vocab_size)
            
            # ëª¨ë¸ ì •ë³´
            model_info = {
                "model_type": self.config.model.model_type,
                "d_model": self.config.model.d_model,
                "num_layers": self.config.model.num_layers,
                "num_heads": self.config.model.num_heads,
                "vocab_size": self.config.model.vocab_size,
                "total_parameters": sum(p.numel() for p in self.model.parameters())
            }
            
            # ì‹œìŠ¤í…œ ì •ë³´
            system_info = {
                "device": "cuda" if torch.cuda.is_available() else "cpu",
                "pytorch_version": torch.__version__,
                "cuda_available": torch.cuda.is_available(),
                "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
            }
            
            return "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.", model_info, system_info
            
        except Exception as e:
            return f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}", {}, {}


def create_gradio_app():
    """Gradio ì•± ìƒì„±"""
    app_instance = GradioApp()
    return app_instance.create_interface()


if __name__ == "__main__":
    # Gradio ì•± ì‹¤í–‰
    app = create_gradio_app()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )
